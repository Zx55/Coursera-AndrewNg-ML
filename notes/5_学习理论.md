# 学习理论

## 模型选择

### 超参数调优

1. 对于一个模型，可以有不同的参数进行选择，这些参数被称作**超参数**(hyperparameter)

2. 为了提高模型的**泛化能力**(Generalization)，通常选取整个数据集的一小部分作为**验证集**(Validation set)来帮助调整超参数

3. 验证误差，这里以均方差函数为例$$J_{cv}(\theta)=\frac{1}{2m_{cv}}\sum_{i=1}^m\left(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)}\right)^2$$其中下标$cv$表示在原数据集中作为交叉验证集的部分

4. 验证集只能帮助选择合适的超参数，但仍然不能评价这个模型的好坏，因为验证的过程本质上是在调整超参数以拟合验证集

### 测试

1. 通常取整个数据集的另外一部分作为**测试集**(Test set)来评估模型

2. 测试误差$$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_\theta(x_{test}^{(i)})-y^{(i)}_{test}\right)^2$$

    下标$test$表示在原数据集中作为测试集的部分

    对于0-1分类问题，还有另外一种计算误差的方法，定义$$err\big(h_\theta(x),y\big)=\begin{cases}1 & misclassification \\ 0 & otherwise\end{cases}$$那么$$J_{test}(\theta)=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err\left(h_\theta(x^{(i)}), y^{(i)}\right)$$

### 数据集划分

1. 通常将$60\%$的数据作为训练集，$20\%$的数据作为验证集，$20\%$的数据作为测试集

2. 通常在划分前要对数据进行随机打乱，然后按照前中后的顺序来划分

### 一般步骤

1. 使用优化算法对**训练集**最小化训练误差$J_{train}(\theta)$来训练多个模型

2. 用这些模型对**验证集**计算交叉验证误差$J_{cv}(\theta)$

3. 选择误差最小的模型

4. 用选出的模型对**测试集**计算测试误差$J_{test}(\theta)$

## 偏差vs方差

### 定义

学习算法的预测误差或者说泛化误差可以分解为三个部分：

1. **偏差**(Bias)描述模型输出结果的期望与样本真实结果的差距

2. **方差**(Variance)描述模型对于给定结果的输出稳定性

3. **噪音**(Noise)

其中我们主要关注偏差和方差，因为噪音属于不可约减的误差

偏差和方差的直观体现如下图

<div align="center"><img src="https://s2.ax1x.com/2019/03/13/AF2m4O.png" style="height:400px"/></div>

### 误差曲线

在模型的训练和验证阶段，训练误差$J_{train}(\theta)$和验证误差$J_{cv}(\theta)$的**误差曲线**可以很好地反应模型偏差和偏差之间的关系

1. 多项式次数对误差曲线的影响

    对于一个多项式模型

    <div align="center"><img src="https://s2.ax1x.com/2019/03/13/AkyV8H.png" style="height:300px"/></div>

    当次数较低时，发生了欠拟合，$J_{train}(\theta)$和$J_{cv}(\theta)$的误差都较大，此时偏差较大，即模型预测结果和实际结果差距大

    当次数较高时，发生了过拟合，$J_{train}(\theta)$小而$J_{cv}(\theta)$大，此时方差较大，即模型对特定的数据预测好，但对另一部分预测差，预测的稳定性不足

2. 正则化参数对误差的影响

    <div align="center"><img src="https://s2.ax1x.com/2019/03/13/Ak6O1J.png" style="height:300px"/></div>

    $\lambda$较小时，对每一项的惩罚太小，发生了过拟合，此时方差大

    $\lambda$较大时，对每一项的惩罚太大，发生了欠拟合，此时偏差大

## 参数调优

### 学习曲线

**学习曲线**(Learning curve)是训练误差$J_{train}(\theta)$和验证误差$J_{cv}(\theta)$与样本数量的关系，是十分有效、直观的分析或是改进学习算法的工具

1. 样本较少时，模型能拟合得很好，此时$J_{train}(\theta)$接近于零，但泛化能力差，$J_{cv}(\theta)$很大

    随着样本增加，$J_{train}(\theta)$也不断增加，但能更好地预测数据，$J_{cv}(\theta)$逐渐下降，如下图

    <div align="center"><img src="https://s2.ax1x.com/2019/03/13/Akc48e.png" style="height:300px"/></div>

2. 在高偏差的情况下，$J_{train}(\theta)$和$J_{cv}(\theta)$都很大且很接近，如下图

    <div align="center"><img src="https://s2.ax1x.com/2019/03/13/AkgJRe.png" style="height:300px"/></div>

    此时一味增加样本量，并不能有效地改进算法的表现

3. 在高方差的情况下，$J_{train}(\theta)$小而$J_{cv}(\theta)$大，如下图

    <div align="center"><img src="https://s2.ax1x.com/2019/03/13/Ak2RpD.png" style="height:300px"/></div>

    这种情况下搜集更多的样本可能对改进算法带来帮助

### 参数调优方法

1. 对于一般模型

    1. 高偏差的情况

        * 寻找更多特征

        * 增加特征项数

        * 降低正则化程度

    2. 高方差的情况

        * 采集更多样本数据

        * 减少特征，去除非主要特征

        * 增加正则化程度

2. 对于神经网络

    1. 使用较小的网络模型，类似多项式的参数较少，计算量小，但是容易导致高偏差和欠拟合

    2. 使用较大的网络模型，类似多项式的参数较多，计算量大，容易导致高方差和过拟合

    通常采用较大的网络模型并采用正则化处理效果更好

## 模型评估

### 偏斜类

如果只使用误差或者预测准确率来评估一个模型的优劣，有时这样做会存在缺陷，这类误差被称作**偏斜类**(Skewed classes)问题，其表现为训练集中某一类的实例占了绝大部分，而其他类很少或是没有

例如在分类问题中，算法都预测结果是占了绝大部分的类型，那么即使误差很低，但也不能将其视为评估算法的有效依据

### 准确率和召回率

将预测结果分为4种情况

||预测为1|预测为0|
|:----:|:----:|:----:|
|**实际为1**|True Positive(**TP**)|False Negative(**FN**)|
|**实际为0**|False Positive(**FP**)|True Negative(**TN**)|

定义**准确率**(Precision)表示预测有多少是正确的，也称作查准率$$P=\frac{TP}{TP+FP}$$

定义**召回率**(Recall)表示正样本有多少被正确预测了，也称作查全率$$R=\frac{TP}{TP+FN}$$

将准确率和召回率作为模型评估可以有效偏斜类问题

### 准确率与召回率的权衡

在学习算法中选择不同的阈值，准确率和召回率的关系会发生改变，很多时候会出现矛盾的情况，因而通常希望准确率和召回率保持相对平衡

定义**F值**$\left(F-\text{score}\right)$，即准确率和召回率的加权调和平均作为综合准确率和召回率的评估量$$F=\frac{(\alpha^2+1)PR}{\alpha^2(P+R)}$$令$\alpha=1$，就是最常用的$F_1$值$\left(F_1-\text{score}\right)$$$F_1=\frac{PR}{P+R}$$