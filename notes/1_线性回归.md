# 线性回归

## 模型建立

$$h_{\theta}(x) = \theta_0 + \theta_1x_1 + \dots + \theta_nx_n$$令$\theta = [\theta_0, \theta_1, \dots, \theta_n]^T$，$x = [x_0, x_1, \dots, x_n]$，其中$x_0=1$

则可以将上式化简为$$h_{\theta}(x) = x\theta$$

## 代价函数

$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left[ h_{\theta}(x^{(i)})-y^{(i)} \right]^2$$其中$m$表示样本数量，$\frac{1}{m}$表示方差的均值

$x^{(i)}$表示第$i$个样本的特征，$y^{(i)}$表示第$i$个样本的目标变量

## 梯度下降

$$\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j}, \; j = 0, 1, \dots, n$$其中$$\begin{aligned}\frac{\partial J}{\partial \theta_j} &= \frac{1}{2m}\sum_{i=1}^{m}\frac{\partial}{\partial \theta_j}\left[ h(x^{(i)})-y^{(i)}\right]^2 \\ &= \frac{1}{2m}\sum_{i=1}^{m}2\left[ h(x^{(i)})-y^{(i)}\right] \frac{\partial}{\partial \theta_j}\left[ h(x^{(i)})-y^{(i)}\right] \\ &= \frac{1}{m}\sum_{i=1}^{m}\left[ h(x^{(i)})-y^{(i)}\right]x_j^{(i)}\end{aligned}$$因此$$\theta_j = \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}\left[ h(x^{(i)})-y^{(i)}\right] x_j^{(i)}$$

## 向量化梯度

偏导项矩阵$$\begin{aligned}\begin{bmatrix}\frac{\partial J}{\partial\theta_0} \\ \frac{\partial J}{\partial\theta_1} \\ \vdots \\ \frac{\partial J}{\partial\theta_n}\end{bmatrix} &= \frac{1}{m} \begin{bmatrix}\sum_{i=1}^m\beta_ix_0^{(i)} \\ \sum_{i=1}^m\beta_ix_1^{(i)} \\ \vdots \\ \sum_{i=1}^m\beta_ix_n^{(i)}\end{bmatrix} \\ &= \frac{1}{m}\begin{bmatrix} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\ | & | & & |\end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2 \\ \vdots \\ \beta_m \end{bmatrix} \\ &= \frac{1}{m}X^T\beta\end{aligned}$$其中$\beta_i=h_\theta\left(x^{(i)}\right)-y^{(i)}$，因此$$\frac{\partial J}{\partial \theta} = \frac{1}{m}X^T\beta$$

## 正规方程

1. 代价函数的矩阵形式为$$\begin{aligned}J(\theta) &= \frac{1}{2m}(X\theta-Y)^T(X\theta-Y) \\ &=\frac{1}{2m}(Y^TY-Y^TX\theta-\theta^TX^TY+\theta^TX^TX\theta)\end{aligned}$$其中$$X=\begin{bmatrix}
x_0^{(1)} & \cdots & x_n^{(1)} \\
\vdots & \ddots & \vdots \\
x_0^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}, Y = \begin{bmatrix}
y^{(1)} \\
\vdots \\
y^{(m)}
\end{bmatrix}$$

2. 当$\frac{dJ}{d\theta}=0$时，$J(\theta)$取到最小值，即$$\frac{dJ}{d\theta}=\frac{1}{2m}(-X^TY+2X^TX\theta)=0$$因此$$\theta=(X^TX)^{-1}X^TY$$

## 多项式回归

1. 对于模型中的高次项如$x^5$，通常将其转化成新特征$x_5$，然后使用线性回归解决问题

2. 对于跨度大的特征，特别是高次项，需要多次迭代才能收敛到最优值

    因此通过**特征标准化**(Normailization)将特征缩放到$[-1, 1]$之间可以有效改进算法，即$$x'=\frac{x - \mu_x}{s_x}$$其中$\mu_x$和$s_x$分别是$x$的均值和标准差