# 神经网络

## 模型概述

1. 在一个如下的神经网络模型中

    <div align="center"><img src="https://s2.ax1x.com/2019/03/01/kbKDFH.png" style="height:300px"/></div>

    最外层是**输入层**(Input layer)，将原始数据输入
    最内层是**输出层**(Output layer)，计算出最终的学习结果$h_\theta(x)$
    中间是**隐藏层**(Hidden layer)，根据自身的模型进行数据处理，并将结果传递给下一层
    神经网络可以有不止一个隐藏层
    除去输出层外，每一层都附带有值为1的**偏差单元**(Bias unit)

2. 对于非线性的拟合问题如多分类，不断扩大特征项会导致计算量过大

    而在神经网络中不会扩展输入层特征的规模，而是通过增加隐藏层，矫正隐藏层中的权值来不断优化特征

## 准备工作

### 符号定义

1. $L$表示神经网络总层数，$S_l$表示每层的神经元个数

2. $a_i^{(l)}$表示第$l$层第$i$个激活单元

    $z_i^{(l)}$表示第$l-1$层到第$l$层的输入

3. $\theta^{(l)}$表示从第$l$层映射到第$l+1$层时的权重矩阵

    其中$\theta^{(l)}_{ij}$表示第$l$层第$j$个激活单元$a^{(l)}_j$到第$l+1$层输入$z^{(l+1)}_i$的坐标变换系数

4. $\delta_i^{(l)}$表示**单个训练实例**在第$l$层第$i$个激活单元$a_i^{(l)}$的误差

    $\Delta^{(l)}$表示**全体训练集**在第$l$层计算得到的误差矩阵

5. $D_{ij}^{(l)}$表示偏导数$\frac{\partial}{\partial \theta^{(l)}_{ij}}J(\theta)$矩阵

### 随机初始化

对神经网络而言，将所有权重参数初始化相同的数被称作**对称权重**(Weight space symmetry)问题，这使前向传播的过程中计算出之后层的所有激活单元都有相同的值

这意味着所有激活单元都以相同的特征为输入，这是一种高度冗余的现象，阻止了神经网络去学习一些有趣的函数，因此要对权重参数进行随机初始化

## 前向传播算法

神经网络层间的神经元通过权重矩阵$\theta^{(l)}$连接，将数据从输入层传递到输出层的过程称为**前向传播**(Feedfoward propagation)

其中一次传播的具体过程为

1. 第$j$层接受上一层的输入$$z^{(j)}=\theta^{(j-1)}a^{(j-1)}$$

2. 通过激活函数$g$的作用产生激活向量$$g(z^{(j)})=a^{(j)}$$

特别地，输入层的激活向量为$$a^{(1)}=x$$

由此将输入由前一层传到下一层直到输出层$$a^{(L)}=g(z^{(L)})=h_\theta(x)$$

## 代价函数

1. 神经网络模型中通常使用交叉熵定义代价函数$$J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^k\left[y_k^{(i)}\log \left(h_\theta(x^{(i)})\right)_k+\left(1-y_k^{(i)}\right)\log \left(1-\left(h_\theta(x^{(i)})\right)_k\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}\left(\theta_{ji}^{(l)}\right)^2$$其中$h_\theta(x)=a^{(L)}$是一个$K$维的向量，即$h_\theta(x)\in \mathbb{R}^K$

    定义$\big(h_\theta(x)\big)_i = a^{(L)}_i$

2. 这个代价函数仍然观察预测结果与真实答案之间的误差，只是对于每个特征输入都会给出$K$个预测，代价函数会选择其中可能性最高的一个，与实际数据$y$进行比较

## 反向传播算法

为了最小化代价函数$J(\theta)$，需要计算偏导数$\frac{\partial}{\partial \theta^{(l)}_{ij}}J(\theta)$，

计算偏导数的过程需要逐层向后计算，因而这个过程被称为**反向传播**(Backpropagation)

### 推导过程

#### 单实例训练集

若训练集只有一个实例$\left\{\left(x^{(1)}, y^{(1)}\right)\right\}$

定义$\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}$为第$l$层神经元的误差项

1. 对于输出层有$$\delta^{(L)}=a^{(L)}-y$$

2. 对于隐藏层，即$l=2,3,\dots,L-1$，有$$\begin{aligned}\delta^{(l)}&=\frac{\partial J}{\partial z^{(l)}} \\ &= \frac{\partial J}{\partial z^{(l+1)}}\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\frac{\partial a^{(l)}}{\partial z^{(l)}} \\ &= \left(\theta^{(l)}\right)^T\delta^{(l+1)}\bigodot g'(z^{(l)})\end{aligned}$$其中$\bigodot$表示各元素相乘

则代价函数$J(\theta)$的偏导数$\Delta^{(l)}_{ij}$为$$\begin{aligned}\Delta^{(l)}_{ij} &= \frac{\partial J}{\partial \theta^{(l)}_{ij}} \\ &=\frac{\partial J}{\partial z^{(l+1)}}\frac{\partial z^{(l+1)}}{\partial \theta^{(l)}_{ij}} \\ &= \sum_{i=1}^m\delta^{(l+1)}a_j^{(l)}\end{aligned}$$

#### 推广到普遍情况

若有训练集$\left\{\left(x^{(1)}, y^{(1)}\right), \dots, \left(x^{(m)}, y^{(m)}\right)\right\}$

考虑正则化处理，则代价函数$J(\theta)$的梯度$D^{(l)}_{ij}$为$$D_{ij}^{(l)}=\begin{cases}\frac{1}{m}\left(\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}\right) & j \neq 0 \\ \frac{1}{m}\Delta^{(l)}_{ij} & j = 0\end{cases}$$

最后更新权重矩阵$$\theta^{(l)}_{ij}=\theta^{(l)}_{ij}+\alpha D^{(l)}_{ij}$$

### 梯度检验

为了检验前向传播以及反向传播算法实现的正确性，通常使用**梯度检验**(Gradient check)来验证

代价函数$J(\theta)$的梯度可以近似计算为$$\frac{J\left(\theta^{(l)}_1, \dots, \theta^{(l)}_i + \epsilon, \dots, \theta^{(l)}_n\right)-J\left(\theta^{(l)}_1, \dots, \theta^{(l)}_i - \epsilon, \dots, \theta^{(l)}_n\right)}{2\epsilon}$$

将其与$D^{(l)}_i$进行比较，若它们近似相等，可以认为正向和反向传播的实现是正确的
但梯度检验的计算量较大，一般不在执行优化算法时进行梯度检验

## 神经网络的训练过程

假定有训练集$\left\{\left(x^{(1)}, y^{(1)}\right), \dots, \left(x^{(m)}, y^{(m)}\right)\right\}$，训练过程如下

1. 选择合适的网络结构，即决定隐藏层的层数和隐藏层中激活单元的个数

2. 参数随机初始化

3. 使用**正向传播算法**计算每个激活值$a_{ij}^{(l)}$

4. 使用**反向传播算法**计算梯度$D_{ij}^{(l)}$并更新权重矩阵$\theta^{(l)}$

5. 使用梯度下降或者其他优化算法最小化代价函数$J(\theta)$