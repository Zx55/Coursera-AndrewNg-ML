# 正则化

## 过拟合和欠拟合

1. 在一个模型中过于强调拟合原始数据，使得假设模型过度严格，而预测新数据的能力可能相应下降的现象称为**过拟合**(Overfitting)

    通常表现为模型中的特征过多、次数过高，如下图<div align="center"><img src="https://s2.ax1x.com/2019/02/10/kUIZZj.png" style="height:300px"/></div>

2. 相反，拟合误差太大，因而预测新数据的能力不佳的现象称为**欠拟合**(Underfitting)

    通常表现为模型中的特征太少、次数太低，如下图<div align="center"><img src="https://s2.ax1x.com/2019/02/10/kUIELQ.png" style="height:300px"/></div>

## 正则化

### 定义

1. **正则化**(Regularization)通过修改代价函数，给所有特征进行惩罚来减小那些影响不大的特征前的参数，从而降低过拟合的现象

    $$J'(\theta)=J(\theta) + \frac{\lambda}{m}\sum_{i=1}^n\theta_j^2$$

    其中$\lambda$称作**正则化参数**

2. 通常不对$\theta_0$进行惩罚

### 线性回归的正则化

代价函数为$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left[\left(h_\theta(x^{(i)})-y^{(i)}\right)^2 + \lambda\sum_{j=1}^n\theta_j^2\right]$$

1. 梯度下降的正则化

    $$\begin{aligned}\theta_0 &= \theta_0-\alpha\frac{1}{m}\sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right] x_0^{(i)} \\ \theta_j &= \theta_j - \alpha \left\{ \frac{1}{m} \sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right]x_j^{(i)} + \frac{\lambda}{m}\theta_j\right\} \\ &=\left(1-\alpha\frac{\lambda}{m}\right) \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right]x_j^{(i)} \end{aligned}$$

2. 正规方程的正则化

    $$\theta = \left(X^TX+\lambda\begin{bmatrix} 0 & & & & & \\ & 1 \\ & & 1 \\ & & & \ddots \\ & & & & 1\end{bmatrix}_{(n + 1) \times (n + 1)}\right)^{-1}X^Ty$$

### 逻辑回归的正则化

代价函数为$$J(\theta)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)}) \log \left(1-h_\theta(x^{(i)})\right)\right] + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$

梯度下降的正则化$$\begin{aligned}\theta_0 &= \theta_0-\alpha\frac{1}{m}\sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right] x_0^{(i)} \\ \theta_j &= \theta_j - \alpha \left\{ \frac{1}{m} \sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right]x_j^{(i)} + \frac{\lambda}{m}\theta_j\right\} \\ &=\left(1-\alpha\frac{\lambda}{m}\right) \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m\left[ h(x^{(i)})-y^{(i)}\right]x_j^{(i)} \end{aligned}$$