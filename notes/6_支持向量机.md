# 支持向量机

## 概述

1. **支持向量机**(Support Vector Machines, SVMs)是一种定义在特征空间上间隔最大的二元线性分类器

2. SVMs通过**核函数**(Kernel)可以将线性推广到非线性分类问题

3. 它的学习策略是间隔最大化，通常都转化为一个求解凸二次规划的最优化问题

## 最大间隔分类器

### 线性分类器

1. 一个线性分类器诸如逻辑回归的学习目标是在$n$维空间中在数据$(x, y)$之间找到一个超平面$f(x)$，这个超平面的方程可以表示为$$f(x) = w^Tx+b=0$$$f(x)>0$对应$y=1$的点；$f(x)<0$对应$y=-1$的点

2. 当超平面距离数据点的间隔越大，则分类的置信度越高
    为了使分类的置信度尽量高，需要让选择的超平面能最大化这个间隔，如下图

    <div align="center"><img src="https://s2.ax1x.com/2019/03/17/AeMQAI.jpg" style="height:300px"/></div>

    因此SVMs也被称作**最大间隔分类器**(Maximum margin classifier)

### 函数间隔和几何间隔

1. 定义函数间隔$\hat{\gamma}$为$$\hat{\gamma}=y(w^Tx+b)=yf(x)$$其中$|w^Tx+b|$表示点$x$到超平面的距离，$y(w^Tx+b)$的正负性来判断分类的正确性

2. 若成比例改变$w$和$b$，则函数间隔可以变得任意大，因此需要对法向量$w$做出约束

    定义$\gamma$为点$x$到分类间隔的距离，则有$$x=x_0+\gamma\frac{w}{||w||}$$其中$x_0$是超平面上的点，满足$f(x_0)=0$，代入有$$\gamma=\frac{f(x)}{||w||}=\frac{w^Tx+b}{||w||}$$因此定义几何间隔$\overset{\sim}\gamma$为$$\overset{\sim}\gamma=y\gamma=\frac{\hat{\gamma}}{||w||}$$

### 优化目标

1. 最大间隔分类器的目标函数可以定义为$$\max\overset{\sim}\gamma, \;\; s.t. \; \; y_i(w^Tx_i+b)=\hat{\gamma}_i\geq\hat{\gamma}$$进行变量替换$w'=w/\hat{\gamma},b'=b/\hat{\gamma}$，有$$\max\frac{1}{||w||}, \;\; s.t. \;\; y_i(w^Tx_i+b)\geq 1$$转换成对偶问题$$\min\frac{1}{2}||w||^2, \;\; s.t. \;\; y_i(w^Tx_i+b)\geq 1$$因此变为了一个求解凸二次规划的最优化问题

2. 如图所示，中间的实线是最优超平面，两条虚线距离它的距离是几何间隔$\overset{\sim}\gamma$

    <div align="center"><img src="https://s2.ax1x.com/2019/03/17/AeMlNt.jpg" style="height:350px"/></div>

    虚线上的点是**支持向量**，都满足$y(w^Tx+b)=1$，对于其他点有$y(w^Tx+b)>1$

## 核函数

### 处理非线性数据

1. 对于线性不可分的情况，通常的做法是将高次项映射到新的特征空间，然后在特征空间中使用线性学习器

    但这样做的一个缺点是特征空间的维数会呈爆炸性增长，若遇到无穷维的情况，就根本无从计算了

2. 使用**核函数**能够简化SVM中的向量在隐式映射得到的特征空间中的内积运算

### 常用核函数

1. **线性核函数**即在原始空间中进行内积运算

2. **高斯核函数**$$K(x_1, x_2)=\exp(-\frac{||x_1-x_2||^2}{2\sigma^2})$$其中$\sigma^2$越大，则函数曲线越平滑，相当于一个低维子空间，伴随高偏差、低方差
    而$\sigma^2$越小，则函数曲线越陡峭，可以将任意数据映射为线性可分，但会伴随低偏差、高方差

## 代价函数

### 合页损失函数

SVMs中用到的损失函数为**合页损失函数**(hinge-loss)，函数定义为$$L(\hat{y})=\max(0, 1 - y\hat{y})$$其中$\hat{y}$是分类器学习的结果

### SVMs的代价函数

1. SVMs的代价函数定义为$$J(\theta)=C\sum_{i=1}^m\left[y^{(i)}L_1(\theta^Tf^{(i)})+(1-y^{(i)})L_0(\theta^Tf^{(i)})\right]+\frac{1}{2}\sum_{j=1}^m\theta_j^2$$其中$f^{(i)}$是$x$经核函数变换到新的特征空间得到的特征

    $L_0$是$y=0$时对应的损失函数，如图<div align="center"><img src="https://s2.ax1x.com/2019/03/17/AewEXn.png" style="height:350px"/></div>

    $L_1$是$y=1$时对应的损失函数，如图<div align="center"><img src="https://s2.ax1x.com/2019/03/17/AewA6s.png" style="height:350px"/></div>

2. 优化目标为$$\min C\sum_{i=1}^m\left[y^{(i)}L_1(\theta^Tf^{(i)})+(1-y^{(i)})L_0(\theta^Tf^{(i)})\right]+\frac{1}{2}\sum_{j=1}^m\theta_j^2$$根据hinge-loss函数的图像可知，经过多次学习后第一项的损失会降低到0，即$$\min\frac{1}{2}\sum^m_{j=1}\theta_j^2=\min\frac{1}{2}||w||^2$$这和从最大间隔出发推导得出的目标是一致的，因此SVMs的学习目标是**最大化间隔**

3. 参数$C$的作用类似于逻辑回归中的$\frac{1}{\lambda}$
    $C$较大时，会导致低偏差、高方差
    $C$较小时，会导致高偏差、低方差

### 假设函数

区别于逻辑回归会输出概率，SVMs的假设函数直接预测1或者是0

$$h_\theta(x)=\begin{cases}1 & \theta^Tx \geq 0 \\ 0 & \theta^Tx<0\end{cases}$$