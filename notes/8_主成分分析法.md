# 主成分分析法

## 概述

1. 数据**降维**(Dimensionality reduction)旨在尽可能减少信息损失的情况下减少原数据集中的特征维数，将存在相关性的特征变为新特征，减少了数据的冗余

2. **主成分分析法**(Principal Component Analysis, PCA)是一种最广泛的数据降维算法，它通过线性变换，将高维数据映射到低维空间中

## 优化目标

设$a$和$b$是原始数据$X$降维后得到的新数据集$Z$的两个训练实例

1. 为了保留更多的信息，原始数据在投影后要尽可能分散，即使投影后的数据方差尽可能大$$\max\frac{1}{m}\sum_i(a_i-\mu_a)^2$$

    特别地，若在降维前进行**特征标准化**，即$\mu_a=0$，那么目标变为了$$\max\frac{1}{m}\sum_ia_i^2$$

2. 为了减少特征的冗余信息，降维后各特征之间要尽可能互不相关，即两两特征之间的协方差要尽可能小$$\min \text{Cov}(a, b)=\min \frac{1}{m}\sum_ia_ib_i$$

3. 以两个实例为例，构造降维后训练集的协方差矩阵$$\begin{aligned}Z_C&=\frac{1}{m}Z^TZ \\ &= \begin{bmatrix}\frac{1}{m}\sum_ia_i^2 & \frac{1}{m}\sum_ia_ib_i \\ \frac{1}{m}\sum_ia_ib_i & \frac{1}{m}\sum_ib_i^2\end{bmatrix}\end{aligned}$$可见上述优化目标等价于协防差矩阵$Y_C$满足对角矩阵

4. 定义映射从$X$到$Y$的映射$P$，即$$Z=XP$$那么有$$\begin{aligned}Z_C&=\frac{1}{m}Z^TZ \\ &= \frac{1}{m}(XP)^TXP \\ &= P^T\left(\frac{1}{m}X^TX\right)P \\ &= P^TX_CP\end{aligned}$$其中$X_C$是实对称矩阵，一定可以正交对角化，$Z_C$是由$X_C$特征值构成的对角矩阵，而所求的映射$P$是满足$X_C$对角化的矩阵，由$X_C$的前$K$个特征向量构成，$K$是降维后的维度

## PCA算法

1. 进行**特征标准化**

2. 计算协方差矩阵$X_C$

3. 对$X_C$进行对角化，计算得到$X_C$的特征向量和特征值构成的对角阵$Z_C$

    选择适当的$K$并取特征向量的前$K$个组成映射$P$，因此降维后的数据为$$Z=XP$$

4. 确保降维后的方差足够大$$\frac{\sum_{i=0}^k\lambda_i}{\sum_{i=0}^n\lambda_i}\ge99\%$$

5. 重构原始数据$$\begin{aligned}X_{approx} &= ZP^{-1} \\ &= ZP^T\end{aligned}$$

## 在监督学习中的应用

1. 在监督学习中，当数据集$(x,y)$特征$x$维度特别大时，模型学习的时间会特别长
    此时可以提取出一个无标签的数据集$x$，然后使用PCA算法将数据降维到$z$，得到一个新的数据集$(z, y)$，再应用监督学习算法

2. 不要默认将PCA作为学习过程的一部分，首先还是从原始特征开始，只有在必要的时候使用PCA算法