# 大规模机器学习

## 梯度下降

1. **批量梯度下降**(Batch gradient descent, BGD)每次使用整个数据集计算偏导更新参数$$\theta_j=\theta_j-\alpha\frac{\partial J}{\partial \theta_j}$$

2. **随机梯度下降**(Stochastic gradient descent, SGD)每次使用单一实例计算偏导，更新参数

    以平方差损失为例，定义单一实例的代价$$J\left(\theta, (x^{(i)}, y^{(i)})\right)=\frac{1}{2}\left(h_\theta(x^{(i)})-y^{(i)}\right)^2$$更新参数$$\theta_j=\theta_j-\alpha\frac{\partial J}{\partial \theta_j}$$

    随机梯度下降并不是每一步都朝着全局最小值前进，而是在最小值附近徘徊，但对于大数据集，速度快于BGD

3. **小批梯度下降**(Mini-Batch gradient descent)介于BGD和SGD之间，每次使用一小部分实例计算偏导更新参数，通常每次梯度下降的实例个数会选择2-100之间

4. **在线学习**(Online learning)指的是对数据流而非离线的静态数据集学习

    类似SGD，每次对单一的实例进行学习，一旦学习完成，就不再需要它了

## 映射化简

**映射化简**(Map Reduce)指的是将一个大数据集分配给多台计算机，让每台计算机处理数据集的一个子集，然后将计算结果汇总求和

## 上限分析

通常一个机器学习应用分为多个阶段，每个阶段都有各自的任务，通过**上限分析**(Ceiling Analysis)可以更好地将时间和精力分配给不同的阶段

在上线分析中，通过手动提供正确结果来替代某一阶段的学习任务，然后查看整体效果提升了多少来决定该阶段的投入